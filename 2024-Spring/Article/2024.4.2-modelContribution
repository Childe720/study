第二章 论文技术基础
1、自回归生成模型
3、拉普拉斯编码介绍
4、多头注意力机制介绍
5、transformer解码器架构
6、rdkit化学包介绍
7、one-hot coding 独热编码：在蛋白质氨基酸图的构建里面采用到的
K-NN算法：也是在氨基酸残基图里面用到的
三、蛋白质口袋导向分子生成模型构建
3.1模型构建
3.1.1蛋白质口袋表征
3.1.2蛋白质编码器
3.1.3分子类药性质编码器
3.1.4分子生成解码器

3.2参数设置
3.2.1模型训练和优化测率
3.2.2训练参数设置

3.3数据集

3.1模型构建
蛋白质口袋，是蛋白质表面与配体分子结合的特定区域。本研究旨在实现一个基于深度学习的以蛋白质口袋为导向的分子生成方法，使得生成的分子能够以高结合亲和力与特定蛋白质结合。即给定一种新的蛋白质p_x，任务是生成一组新的分子{{m}_x^k,k=1,2,...}，这组分子能够以高结合亲和力与p_x结合，并具有所需的相关类药性质d_x。以下是框架的具体构建。
3.1.1蛋白质口袋表征
为了获取蛋白质口袋的结构信息，需要将其表征为计算机能够识别处理的内容。给定一个3D蛋白质结构，为了表征该蛋白质的结合口袋，为其分别构建一个氨基酸残基图和一个原子图，分别代表了该蛋白质口袋的粗粒度表示和细粒度表示。
对氨基酸残基图的构建来说，根据20种氨基酸残基的类型，将每个残基视作一个图节点，为了将这些点的离散取值扩展到欧式空间，以方便合理计算点之间的距离，每个节点采用独热编码。之后，采用K=48的K-NN算法进行两个节点之间边的构造。
对原子图的构建而言，选取了常用的6种原子类型：H, C, N, O, S, P，同样地采用独热编码，每个原子作为一个图节点，并额外添加一位对该原子是否为骨架进行注释（1表示该原子在骨架中的位置，0表示不在）。之后，采用K=30的K-NN算法进行两个节点之间边的构造。
因为采用的解码器Transformer，其位置编码很难定义图中节点的位置，为了获取节点的唯一位置表示，引入了拉普拉斯位置编码，为整个蛋白质口袋图增加了位置信息。
3.1.2蛋白质口袋编码
在解码器生成分子之前，需要获得一个完整的蛋白质结合口袋表示，将上述氨基酸残基图和原子图结合。在这个过程中，为了使模型能够更好地捕捉口袋结构和功能信息，减少噪声干扰，获得节点及节点之间的准确信息，捕获不同节点间的交互模式，丰富单个节点内容，提高分子生成性能，需要对两个图进行边缘增强并使用多头注意力机制。
将两图分别嵌入一个并行编码器中，每个编码器包含一个多个串联编码单元，每个单元又包含边缘增强编码块和多头注意力机制块。经过注意力机制更新后，节点表示会通过一个前馈网络（FNN）进行进一步的处理。
在这个过程中，虽然对氨基酸残基图和原子图进行并行节点增强处理，但为了更好地捕获蛋白质口袋的天然层次结构，需要对二者进行融合。我们将氨基酸残基图视为粗粒度表示，将原子图视为细粒度表示，采用交叉注意力块对两个表示进行融合。因为是氨基酸缔合其包含的原子，故而将交叉注意力块输出的结果单向用于更新氨基酸残基图（粗粒度）的表示。此外，为避免信息冗余，仅只对中间编码单元和最后一个编码单元采用交叉注意力机制进行粗粒度和细粒度的交叉融合，并更新粗粒度信息。
最终，更新后的节点表示被用作下一个编码单元的输入，或者作为整个图编码过程的输出。对氨基酸残基图和原子图进行并行编码后的节点表示矩阵被垂直堆叠，输入最后的分子生成解码器中。
3.1.3分子类药性质编码器
该模块根据数据集对与蛋白质口袋结合的配体分子进行编码，将分子编码为smiles序列，并同时编码该分子的类药性质以及与相关蛋白质口袋的结合亲和力等。
编码smiles序列时，依据的是一个词汇表，该词汇表包含了smiles字符串中的子字符串，使得最终的smiles字符串由这些子序列构成。其中，字符“&”作为一个smiles序列的起始标记，用于解码时的开始标签；字符“$”是一个smiles序列结束的标记，用于解码时提示模型解码结束的标签；字符“^”用于确保不同长度的序列在输入模型时具有相同的长度，较短的序列会在其末尾添加该填充标记，直到它们达到所需的长度。这样，模型就可以处理具有固定长度的输入，简化了计算过程。
因为采用的生成模型自回归生成模型本质上是一个条件生成模型，通过一个依据条件概率的迭代模型，每一步都以之前的值作为条件生成，因此，需要将性质堆叠在smiles序列之前作为生成条件。具体而言，就是将蛋白质口袋与配体分子的结合分数以及分子的类药性质编码为一个特征向量，拼接在smiles序列特征的前面作为属性控制条件，成为生成下一个分子的第一个条件，保证生成的分子具有可控的相关性质。
为了区分分子的性质字符串和smiles序列，添加了一个token,对token分配二进制类型指示符即用token=1表示分子的性质，而token=0指示分子的smiles序列。
3.1.4分子生成解码器
解码器采用了与原始Transformer解码器类似的自回归解码架构。（具体技术详见2.2介绍）。设置解码序列的最大长度为smiles序列的最长长度即200字符串。
在这个解码器中包含了多个串联的解码单元，每个单元又包括了一个掩码多头注意力机制块、一个多头注意力机制块和一个多层感知器。掩码多头注意力机制块用于接收来自小分子的编码。解码时，因为来自编码的序列会一次性输出给Transformer，为了保证Transformer的输出不会因为传入词的多少而改变，防止前面先解码的词具备后面解码词的上下文信息，所以在进行模型训练时，加入了掩码机制，避免信息泄露。多头注意力机制块将掩码后的分子作为查询输入，将蛋白质的氨基酸残基和原子分别作为注意力层中的键和值输入，以此计算分子和蛋白质的氨基酸残基和原子之间的结合分数。输入前向传播中。
分子生成是通过一个自回归的解码过程完成的。该过程从上一步编码的序列的最初开始（即分子的相关性质），逐一迭代将解码出的下一个分子添加到序列中。在生成过程中，模型根据学习到的条件概率分布生成新分子，直到解码到序列的结束标志为止，通过top-k高条件概率获得完整生成的标记字符串。（Top-k是一种解码的策略。它从排名前k的token中进行抽样，允许其他分数或概率较高的token 也有机会被选中。在很多情况下，这种抽样带来的随机性有助于提高生成质量。top-k 采样的思路是，在每一步，只从概率最高的 k 个单词中进行随机采样，而不考虑其他低概率的单词。这样可以避免采样到一些不合适或不相关的单词，同时也可以保留一些有趣或有创意的单词。）如此得到的新序列，就是生成的新分子的smiles序列串。最终生成了面向蛋白质口袋的具有高结合亲和力和所需特性的新分子。
3.2参数设置
3.2.1模型训练和优化测率
（1）损失函数选择
损失函数无疑是机器学习和深度学习效果验证的核心检验功能，用于评估模型预测值与实际值之间的差异。与传统的分类或回归任务不同，生成模型的目标不是对现有数据进行分类或者回归任务，它们的目标是通过学习输入数据的模式和分布规律生成与训练数据类似但又不完全相同的新样本。
生成模型的损失函数主要承担了以下几个功能：1、引导模型训练方向： 损失函数的值告诉了优化算法应该如何更新模型的参数，以使模型的预测结果更接近实际数据。2、评估生成模型的性能： 生成模型的损失函数值可以作为一个指标，用来评估模型的性能。通常情况下，我们希望损失函数越小越好，因为这意味着模型的预测结果越接近实际数据。3、防止过拟合： 通过选择合适的损失函数和正则化项，可以降低模型对训练数据的过拟合程度，提高模型在新数据上的泛化能力。4、梯度下降优化： 损失函数的梯度是优化算法（如随机梯度下降）的依据，它指导了参数的更新方向和幅度，从而使得模型逐渐收敛到最优解。生成模型损失函数主要是帮助模型学习到适合任务的参数配置，从而生成更符合实际数据分布的新样本。常见生成模型的损失函数包括以下几种：
1、负对数似然损失（Negative Log Likelihood Loss，NLL Loss）：
负对数似然损失是一种常用于概率模型训练的损失函数，特别是在分类和生成模型中。它用于衡量模型的预测概率分布与实际样本分布之间的差异。在生成模型中，如变分自编码器（VAE）或生成对抗网络（GAN）等，通常使用 NLL Loss 作为训练的损失函数。它帮助模型学习生成符合实际数据分布的新样本。假设我们有一个概率模型，它给出了样本属于每个类别的概率分布为p(y|x),其中y是类别标签, x是样本。对于一个样本x_i，它的实际标签是y_i，那么负对数似然损失可以定义为：\bigm{NLL(X}_i)=-log{\left(p\left({y_i|x}_i\right))\right.}
2、重构误差（Reconstruction Loss）:
重构误差是指在生成模型或自编码器中，衡量模型重构输入数据的能力的指标。它表示模型在将输入数据编码为潜在空间表示后，再解码回原始输入时产生的误差。在自编码器中，重构误差通常是训练过程的一个重要组成部分。自编码器的目标是最小化输入数据与重构数据之间的差异，以便学习到一个有效的特征表示。重构误差可以用来指导模型的训练，使得模型能够在保留关键信息的同时，降低噪声或不必要的细节。通常，重构误差的计算方式取决于所使用的模型和任务。对于像变分自编码器（VAE）这样的模型，重构误差通常由负对数似然损失（Negative Log Likelihood Loss）来度量。在其他生成模型中，可能会使用不同的损失函数来衡量重构误差。具体的计算方式取决于所使用的模型和任务。以自编码器为例，其重构误差通常由以下公式表示： ReconstructionLoss=\sum_{i-1}^{n}{||x_i-\bar{x_i}||}^2
3、KL散度（Kullback-Leibler Divergence，KLD）
KL散度也称为相对熵，是信息论中用于衡量两个概率分布之间的差异的一种指标。在生成模型中，特别是在变分自编码器（VAE）等模型中，KL散度通常用于衡量两个概率分布之间的差异。具体来说，它用于度量在一个概率分布下用第二个概率分布来表示所需的额外信息量。假设有两个概率分布P(x)和Q(x)(x表示随机变量)，它们分别描述了同一个事件的不同观测结果的概率分布。
4、对抗损失（Adversarial Loss）
对抗损失是一种用于训练生成对抗网络（GANs）的损失函数。它在GAN模型中起到了至关重要的作用。在GAN中，通常包括两个主要的组件：生成器（Generator）： 它试图生成与真实数据样本相似的虚假数据样本。判别器（Discriminator）： 它试图区分真实数据样本和由生成器生成的虚假数据样本。对抗损失的核心思想是通过将生成器和判别器对抗训练，来达到使生成器生成逼真样本的目的。具体来说，对抗损失由两部分组成：生成器损失： 这一部分的目标是欺骗判别器，使其将生成器生成的样本误认为是真实数据。判别器损失： 这一部分的目标是准确地区分真实数据和虚假数据。对抗损失是GAN模型训练中非常重要的一部分，它使得生成器能够逐渐提升生成样本的质量，从而达到生成真实样本的目的。
本模型采用了交叉熵损失函数（Cross Entropy Loss）。交叉熵刻画的是两个概率分布的距离，也就是说交叉熵值越小（相对熵的值越小），两个概率分布越接近。
对于生成模型来说，特别是当模型的目标是生成具有特定属性的数据（如分子结构）时，模型通常被训练来预测分子中下一个原子或键的类型和位置，在每个时间步，模型会输出一个概率分布，表示下一个可能添加的原子或键的类型。交叉熵损失函数则用于计算模型预测的概率分布与真实分子结构对应的概率分布之间的差异。通过最小化交叉熵损失，模型可以逐渐学习如何生成与真实分子结构更相似的序列。这种损失函数鼓励模型在每一步都做出最有可能导致最终生成正确分子的预测，它有助于模型在训练过程中更好地学习数据的特征，使得模型可以逐步调整其参数，以产生更接近真实样本的输出，从而优化预测结果。
交叉熵损失函数数学公式表示如下：
H\left(P^\ast|P\right)=-\sum_{i}{P^\ast(i)logP(i)}
其中，P^\ast(i)表示真实类的分布，而P(i)表示预测类的分布。
（2）优化策略

3.2.2训练参数设置

3.3数据集
模型采用的是公开数据集CrossDocked。该数据集来源于由Paul G. Francoeur等提出的CrossDocked2020。Paul G. Francoeur等人针对预测蛋白质-配体结合亲和力的模型评估，提出了基于结构的机器学习的标准数据集CrossDocked2020，该数据集最初包含了2250万对不同质量水平的对接蛋白配体。在此基础上，Luo等人对CrossDocked2020进行处理，得到了本研究领域广泛使用的CrossDocked数据集。
具体处理方法如下：首先，Luo等人将数据点结合姿态RMSD大于1Å的过滤掉，得到由184057个蛋白质-配体对组成的数据集。该数据集涉及2922个蛋白质口袋和13839个配体分子，每个蛋白质-配体对的对接分数都通过Autodock Vina进行测量。之后，通过MMseqs2以30%的序列同一性水平对蛋白质进行聚类，使得来自不同聚类的两种蛋白质具有≤30%的序列相同性（即显著不同）。然后，从这些集群中选取几个集群（如选取25个集群）作为测试集群，其余集群分别作为训练集群。接着，随机抽取100 000个蛋白质-配体对来构建模型，其中99 000对标记为训练对，1000对标记为验证对。最后，从测试集群中随机选择了100种蛋白质（涉及约18K蛋白质-配体对）作为测试蛋白质进行参考，并评估了与训练蛋白质显著不同的分子生成相关蛋白质的性能。如此，得到了CrossDocked数据集。
我通过链接https://drive.google.com/drive/folders/1CzwxmTpjbrt83z_wBzcQncq840VDPurM下载得到数据集crosstoked_pocket10.tar.gz和它的拆分文件split_by_name.pt。
