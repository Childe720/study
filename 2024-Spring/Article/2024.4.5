第二章 论文技术基础
1、自回归生成模型
3、拉普拉斯编码介绍
4、多头注意力机制介绍
5、transformer解码器架构
6、rdkit化学包介绍
7、one-hot coding 独热编码：在蛋白质氨基酸图的构建里面采用到的
K-NN算法：也是在氨基酸残基图里面用到的
2.1自回归生成模型
2.2encoder-decoder
多头注意力机制、transformer解码器、one-hot编码机制、k-NN算法
2.3rdkit化学包介绍
我觉得这里不介绍也行，放在第四部分也蛮好的，省得这里凑字数太明显了。

什么是生成模型？有哪些主流的生成模型，
上面这个视情况而定，我觉得不用系统介绍所有生成模型，倒是可以把知乎上那张图模仿着画一下，就当作是总结了。

2024.4.2
明早来把优化器看明白，看看有没有机会改成adamW
下午来看下技术部分要怎么介绍

2024.4.4
技术路线介绍的思路：
硬件技术：GPU训练规格，使用的软件
模型技术路线：
1、简单介绍自回归生成模型。
或者可以添加一下后文这个自回归编码器是如何体现的
2.1、蛋白质口袋编码器
2.2、小分子编码器
2.3、蛋白质-配体解码器


我看来一下这个编码和解码架构基本采用的是和transformer一样的架构进行的
老火了，感觉自己写的文章有点空。
先不管了，第二章这里我觉得把transformer的编码和解码结构讲清楚就好。


2.1生成模型——自回归模型
深度生成模型的数据生成过程，可以看成是将一个先验分布的采样点Z变换成数据分布的采样点X的过程。生成模型在传统机器学习中具有悠久的历史，常见的模型包括：Autoregressive models、VAE、GAN、flow、DDPM等。下图是对各个生成模型的汇总表示：
 
图源：https://m.thepaper.cn/baijiahao_19541556
在本实验中，笔者采用的是生成模型中的自回归模型。自回归模型（Autoregressive Model）是一种用于时间序列数据分析的统计模型。它基于时间序列中的自相关性，将当前变量的值与过去若干时刻的值进行线性组合，用以预测未来的值。自回归模型的一般数学表达式归纳如下：X_t=c+\sum_{i=1}^{p}{\varphi_iX_{t-i}}+\varepsilon_t。其中：c是常数项；\varepsilon_t被假设为平均数等于0，标准差等于\delta的随机误差值；\delta被假设为对于任何的t都不变。表达式解释为：任何一个X的期望值等于一个或数个落后期的线性组合，加上常数项和随机误差。最早将自回归模型引入生成模型的可以追溯到2016年由DeepMind提出的PixelCNN模型。生成模型中的自回归模型是一种基于序列数据的生成式框架，它通过学习序列中的依赖关系预测新数据点，生成新的序列。该模型的关键是确定滞后阶数，即过去时刻的数量。选取合适的滞后阶数需要通过统计方法或模型选择准则进行评估。通常采用最小二乘法进行参数估计，通过最小化观测值与模型预测值之间的残差平方和来确定参数的值。在生成模型中，通常将自回归模型的总结为如下表达式：p(x)=p(x_0)\prod_{i=1}^{D}{p(x_i|x_{<i}})。

在对蛋白质口袋进行表征时，对氨基酸残基图和原子图的节点进行了one-hot编码。下面是one-hot编码的介绍：
One-Hot编码，又称为一位有效编码，为了便于loss值的计算，需要在图中将互相独立的节点标签表示为互相独立的数字，并且保证编码后数字之间的距离也相等，引入了one-hot(独热编码) , 用二进制向量来表征这种离散的分类标签。编码采用N位状态寄存器来对N个状态进行编码，每个状态都有独立的寄存器位，并且在任意时候只有一位有效。One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值，然后每个整数值被表示为二进制向量，除了整数的索引之外，其它都是零值，该位被标记为1。在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。而我们使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，让特征之间的距离计算更加合理。在代码中，utils/transforms.py对蛋白质中的氨基酸和原子两种信息进行了独热编码处理。
2.2“编码器-解码器”框架
Encoder-Decoder是一个模型构架，是一类算法统称，并不是特指某一个具体的算法，在这个框架下可以使用不同的算法来解决不同的任务。首先，编码（encode）由一个编码器将输入序列转化成一个固定维度的稠密向量，解码（decode）阶段将这个激活状态生成目标译文。即将一个长度不确定的序列映射到另一个长度也未知的序列。编码-解码模型模仿人脑的翻译过程，并非机械的逐字翻译，而是先理解意思再创作的过程。它的核心思路是先用编码器将输入序列转换为一个固定长度的向量，再通过解码器将这个向量转换为输出序列。
此处重点介绍本任务中采用的Transformer模型。
Transformer是一种深度学习模型结构，最初由Google公司提出，该模型最早应用于机器翻译领域，利用其自身的self-attention机制实现了快速并行，解决了RNN训练慢的问题。并且Transformer可以将模型增加到非常深的深度，充分发挥深度学习的特性，提升模型准确率，其网络核心结构如图所示： 
 
2.2.1基于transformer的编码器——多头注意力机制
Transformer 结构中最核心的就是多头注意力机制(Multi-head Attention)。
自注意力机制模仿和扩展了人类大脑的机制，通过选择性地聚焦于重要信息，被应用于各个层中，从而提高模型对输入数据的理解和处理能力。自注意力机制可以用如下数学公式表示：
                                      \ Attention\left(Q,K_i,V_i\right)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V_{i\ \ \ \ \ \ \ \ \ \ }\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ #\ \ 式
公式含义为将Q与K矩阵内积后进行归一化操作，再与V矩阵进行乘积得到权重求和。公式中d_k代表Key向量的维度，是防止点乘之后矩阵数据太大，用来降低运算量的。
注意力模型采用查询-键-值（Query-Key-Value，QKV）模式。这个模式概念最初是针对自然语言处理任务提出的。在注意力机制中，Q（Query，查询）、K（Key，键）和V（Value，值）都是以矩阵形式存在的重要向量。具体而言，Q（Query，查询）矩阵代表想要获取的信息的请求。它可能源于一个单词、一个句子或是一整段文本，是我们在处理数据时所关心的焦点。K（Key，键）矩阵则包含了一系列与查询相关的特征向量。这些向量用于评估查询与序列中其他各个元素之间的关联程度或相似度。通常，K的数量与序列中元素的数量是一一对应的，确保每一个序列元素都能与查询进行比较。V（Value，值）矩阵则存储了与每个键相对应的元素的特征向量。这些向量是对键所对应元素更深入的描述，包含了丰富的信息，以支持后续的计算和推理。
自注意力机制实质上就是一个寻址过程，通过给定一个任务相关的查询 Query 向量 Q，通过计算与 Key 的注意力分布并附加在 Value 上，从而计算 Attention Value。 这个过程实际上是 Attention 缓解神经网络复杂度的体现，不需要将所有的 N 个输入都输入到神经网络进行计算，而是选择一些与任务相关的信息输入神经网络，与 RNN 中的门控机制思想类似。
当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系。因此，允许注意力机制组合使用查询、键和值的不同子空间表示。那么与其只使用单独一个注意力汇聚，我们可以用独立学习得到的n组不同的线性投影（linear projections）来变换查询、键和值。然后，这n组变换后的查询、键和值将并行地送到注意力汇聚中。最后，将这n个注意力汇聚的输出拼接在一起，通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为多头注意力(multihead attention)。对于n个注意力汇聚输出，每一个注意力汇聚都被称作一个头（head）。如下图所示，为多头注意力机制的模型。
 
下展示了使用全连接层来实现可学习的线性变换的多头注意力。
 
图源：https://zh.d2l.ai/chapter_attention-mechanisms/multihead-attention.html#fig-multi-head-attention之图10.5.1
多头注意力机制的公式表示如下：
MultiHeadQ,K,V=Concathead1,…,headhWO where headi=Attention(QWiQ,KWiK,VWiV)式
下图是多头注意力机制的计算过程：
 
2.2.2基于transformer的解码器
Transformer的解码器包含了两个多头注意力层。第一个多头注意力层采用了掩码操作。第二个多头注意力层的K, V矩阵使用编码器的编码信息矩阵C进行计算，而Q使用上一个解码单元的输出计算。最后有一个Softmax层计算下一个翻译单词的概率。
2.2.2.1 Masked Multi-Head Attention
解码器每个解码单元的第一个多头注意力机制采用了掩码机制。在机器翻译或文本生成任务中，我们经常需要预测下一个单词出现的概率，这类任务我们一次只能看到一个单词。此时注意力只能放在下一个词上，不能放在第二个词或后面的词上。简而言之，注意力不能有非平凡的超对角线分量。即在翻译的过程中是顺序翻译的，即翻译完第i个单词，才可以翻译第i+1个单词，这样可以防止第 i 个单词知道 i+1 个单词之后的信息。我们可以通过添加掩码矩阵来修正注意力，以消除神经网络对未来的了解：\bigm                                       AttentionQ,K,V=softmaxQKTdk+MV                #  式
其中M为掩码矩阵，其定义为：\bigm                                       M=(mi,j)i,j=0n                #  式
                                      mi,j=-∞,&i<j0,&i≥j               #  式
2.2.2.2解码器架构
解码器的目的是产生输出。在原作论文 Attention is All You Need 中，解码器被用于句子翻译，例如编码器接受中文句子，解码器会把它翻译成英文。在分子生成任务中，本模型接受来自蛋白质口袋和小分子的两种编码，之后将二者结合解码为新的smiles序列。
解码器每个解码单元的第二个多头注意力模块主要的区别在于其中自注意力的 K, V矩阵不是使用上一个解码单元的输出的计算，而是使用编码器的编码信息矩阵 C 计算的。根据 编码器的输出 C计算得到 K, V，根据上一个解码单元的输出 Z 计算 Q (如果是第一个 解码单元则使用输入矩阵 X 进行计算)，后续的计算方法与之前描述的一致。这样做的好处是在 解码的时候，每一位单词都可以利用到来自编码器的所有单词的信息 (这些信息无需进行掩码操作)。
解码器解码单元的最后部分是利用 Softmax ，根据输出矩阵的每一行预测下一个单词。
